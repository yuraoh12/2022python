{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27c3f44",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n",
      "     ---------------------------------------- 6.5/6.5 MB 30.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "     ------------------------------------- 384.9/384.9 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed async-generator-1.10 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.9.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e219cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_argument('headless')\n",
    "service = ChromeService(executable_path='C:/Users/Administrator/jupyter notebook/chromedriver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get('https://n.news.naver.com/article/055/0001052871?ntype=RANKING')\n",
    "\n",
    "time.sleep(1)\n",
    "# find\n",
    "cbox_chart = driver.find_element(By.CLASS_NAME, 'u_cbox_chart_wrap')\n",
    "cbox_chart_per = cbox_chart.find_element(By.CLASS_NAME, 'u_cbox_chart_per')\n",
    "\n",
    "print(cbox_chart_per.text)\n",
    "\n",
    "time.sleep(10)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8047b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'남자': '66%', '여자': '34%', '10대': '1%', '20대': '3%', '30대': '18%', '40대': '45%', '50대': '26%', '60대': '7%'}\n"
     ]
    }
   ],
   "source": [
    "# 실습1\n",
    "# 뉴스의 링크를 넘기면 해당 뉴스의 댓글 통계 정보를 딕셔너리로 반환하는 함수 만들기\n",
    "\n",
    "def get_stat_gen(url) :\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    chart_sex = driver.find_element(By.CLASS_NAME, 'u_cbox_chart_sex')\n",
    "    per_list = chart_sex.find_elements(By.CLASS_NAME, 'u_cbox_chart_per')\n",
    "    \n",
    "    dic_keys = ['남자', '여자']\n",
    "    sex_dic = {}\n",
    "    \n",
    "    for key, per in zip(dic_keys, per_list) :\n",
    "        sex_dic[key] = per.text\n",
    "    \n",
    "    return sex_dic\n",
    "\n",
    "def get_stat_age(url) :\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    chart_age = driver.find_element(By.CLASS_NAME, 'u_cbox_chart_age')    \n",
    "    per_list = chart_age.find_elements(By.CLASS_NAME, 'u_cbox_chart_per')\n",
    "    \n",
    "    age_keys = ['10대', '20대', '30대', '40대', '50대', '60대']\n",
    "    age_dic = {}\n",
    "    \n",
    "    for key, per in zip(age_keys, per_list) :\n",
    "        age_dic[key] = per.text\n",
    "    \n",
    "    return age_dic\n",
    "    \n",
    "def get_reply_stat(url) :\n",
    "        \n",
    "    gen_info = get_stat_gen(url)    \n",
    "    age_info = get_stat_age(url)\n",
    "    \n",
    "    gen_info.update(age_info)\n",
    "    \n",
    "    return gen_info\n",
    "    \n",
    "url = 'https://n.news.naver.com/article/055/0001052871?ntype=RANKING'\n",
    "stat_info = get_reply_stat(url)\n",
    "print(stat_info)\n",
    "\n",
    "\n",
    "# {'남자' : '68%', '여자' : '32%', '10대' : '1%', '20대' : '4%', '30대' : '17%',\n",
    "#  '40대' : '45%', '50대' : '26%', '60대' : '8%' }\n",
    "\n",
    "# 실습2\n",
    "# SBS, MBC, KBS 세 언론사의 모든 랭킹뉴스 통계 정보를 json 파일로 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습3\n",
    "# 각 뉴스의 댓글 수를 스크랩해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09718f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습4\n",
    "# 수집한 뉴스데이터를 이용해 다음 문제를 풀어주세요.\n",
    "## 1. 20대가 가장 많이 본(댓글을 많이 작성한) 뉴스\n",
    "\n",
    "## 2. 가장 댓글이 적은 뉴스의 댓글 수와 뉴스 링크, 뉴스 번호\n",
    "\n",
    "## 3. 언론사 번호와 언론사 이름으로 구성된 데이터 프레임을 만들고 merge를 이용해 각 뉴스에 언론사 이름을 붙여주기\n",
    "\n",
    "## 4. 각 언론사별 평균 댓글 수(댓글 수로 내림차순 정렬)\n",
    "\n",
    "## 5. 여성의 댓글수가 가장 많은 언론사\n",
    "\n",
    "## 6. 각 언론사 별 댓글을 많이 작성한 연령대 top3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
